{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RandomAnass/Analysis-of-new-phenomena-in-deep-learning/blob/main/Fashion_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "H2xhSZUQmMZB",
        "outputId": "0c3fc168-b00e-495d-8e31-ceea19d3829c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Normalize the pixel values between 0 and 1\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Reshape the input data to add a channel dimension (needed for convolutional models)\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)"
      ],
      "metadata": {
        "id": "5iR8kXaLmUss"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Plot examples of images from the Fashion MNIST dataset\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal',\n",
        "               'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# Select random images from the test set\n",
        "num_images = 5\n",
        "random_indexes = np.random.choice(len(x_test), num_images, replace=False)\n",
        "images = x_test[random_indexes]\n",
        "labels = y_test[random_indexes]\n",
        "\n",
        "# Plot the images\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(num_images):\n",
        "    plt.subplot(1, num_images, i+1)\n",
        "    plt.imshow(images[i].reshape(28, 28), cmap='gray')\n",
        "    plt.title(class_names[np.argmax(labels[i])])\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yo-Sy828mVUV",
        "outputId": "6650953c-d89d-4cc0-d10f-480e09f75362",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "id": "sJSakqVVm895",
        "outputId": "65ef4bf5-45f7-4249-a56c-98b50e5019b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install adversarial-robustness-toolbox"
      ],
      "metadata": {
        "id": "uCSEEouRnGEk",
        "outputId": "b0f8511a-a582-4c76-9b81-1de68e03ef6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting adversarial-robustness-toolbox\n",
            "  Downloading adversarial_robustness_toolbox-1.14.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.10.1)\n",
            "Collecting scikit-learn<1.2.0,>=0.22.2 (from adversarial-robustness-toolbox)\n",
            "  Downloading scikit_learn-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (67.7.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (4.65.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.2.0,>=0.22.2->adversarial-robustness-toolbox) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.2.0,>=0.22.2->adversarial-robustness-toolbox) (3.1.0)\n",
            "Installing collected packages: scikit-learn, adversarial-robustness-toolbox\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed adversarial-robustness-toolbox-1.14.1 scikit-learn-1.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, ReLU, GlobalAveragePooling2D, Dense, Lambda, GlobalMaxPooling2D,Flatten\n",
        "from art.estimators.classification import TensorFlowV2Classifier\n",
        "from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescentTensorFlowV2\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from art.utils import load_mnist\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "wDlYvmGlnJXc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "tf.config.list_physical_devices(\"GPU\")\n"
      ],
      "metadata": {
        "id": "L1Yn0tGTnKGj",
        "outputId": "4fdcfa73-5675-42b9-e6c6-3ff49776ce32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def circular_padding(x, padding_size):\n",
        "    # Perform circular padding on the input tensor\n",
        "    return tf.pad(x, [[0, 0], [padding_size, padding_size], [padding_size, padding_size], [0, 0]], mode='SYMMETRIC')\n",
        "\n",
        "def simple_Conv(n_hidden, kernel_size=28, padding_size=-1):\n",
        "    if padding_size == -1:\n",
        "        padding_size = kernel_size // 2\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Lambda(lambda x: circular_padding(x, padding_size), input_shape=(28, 28, 1)))\n",
        "    model.add(Conv2D(n_hidden, kernel_size=kernel_size, padding='valid'))\n",
        "    model.add(ReLU())\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(10))\n",
        "\n",
        "    return model\n",
        "\n",
        "def simple_Conv_NL(n_hidden,kernel_size=28):\n",
        "    \"\"\" no lambda \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(n_hidden, kernel_size=kernel_size, padding='same', input_shape=(28, 28, 1), activation='relu'))\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(10))\n",
        "\n",
        "    return model\n",
        "\n",
        "def simple_FC(n_hidden):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28, 28)))\n",
        "    model.add(Dense(n_hidden, activation=\"relu\"))\n",
        "    model.add(Dense(10))\n",
        "\n",
        "    return model\n",
        "\n",
        "def simple_Conv_max(n_hidden, kernel_size=28):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(n_hidden, kernel_size=kernel_size, padding='same', input_shape=(28, 28, 1), activation='relu'))\n",
        "    model.add(GlobalMaxPooling2D())\n",
        "    model.add(Dense(10))\n",
        "\n",
        "    return model\n",
        "\n",
        "def simple_Conv_max(n_hidden, kernel_size=28):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(n_hidden, kernel_size=kernel_size, padding='same', input_shape=(28, 28, 1), activation='relu'))\n",
        "    model.add(GlobalMaxPooling2D())\n",
        "    model.add(Dense(10))\n",
        "\n",
        "    return model\n",
        "\n",
        "def simple_Conv_2():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "def simple_FC_2():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Flatten(input_shape=(28, 28)),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def simple__RNN():\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Reshape((28, 28), input_shape=(28, 28, 1)),\n",
        "        layers.LSTM(128, return_sequences=True),\n",
        "        layers.LSTM(128),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "model_names = {'simple_FC':simple_FC, 'simple_Conv':simple_Conv, 'simple_Conv_NL':simple_Conv_NL, \n",
        "               'simple_Conv_max':simple_Conv_max, 'simple__RNN':simple__RNN , 'simple_FC_2':simple_FC_2, 'simple_Conv_2':simple_Conv_2}"
      ],
      "metadata": {
        "id": "9FA_uQcUnYmh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist(np.argmax(y_train, axis=1))\n",
        "plt.hist(np.argmax(y_test, axis=1))"
      ],
      "metadata": {
        "id": "PD_nzFWcnenI",
        "outputId": "47107a63-6e90-4b49-879f-58b98dbf64e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1000., 1000., 1000., 1000., 1000., 1000., 1000., 1000., 1000.,\n",
              "        1000.]),\n",
              " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
              " <BarContainer object of 10 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padding_sizes = [0, 2, 4, 6, 8, 10, 12, 14]\n",
        "#padding_size=-1\n",
        "padding_size = 0 #padding_sizes[3]\n",
        "n_hidden = 1024 #1000\n",
        "kernel_size=28"
      ],
      "metadata": {
        "id": "vPQRDtT1ni7H"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from art.estimators.classification import TensorFlowV2Classifier\n",
        "\n",
        "def train_step(model, optimizer, loss_object, images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(images, training=True)\n",
        "        loss = loss_object(labels, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "def create_art_classifier(model_creator, x_train, y_train, batch_size=200, nb_epochs=30, **kwargs): # nb_epochs=200\n",
        "    # Create the CNN model and optimizer\n",
        "    model = model_creator(**kwargs)\n",
        "    optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=0.1, momentum=0.9, decay=5e-4)\n",
        "    loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    model.compile(optimizer=optimizer, loss=loss_object, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    # Create the ART classifier\n",
        "    classifier = TensorFlowV2Classifier(\n",
        "        model=model,\n",
        "        loss_object=loss_object,\n",
        "        train_step=train_step,\n",
        "        nb_classes=10,\n",
        "        input_shape=(28, 28, 1),\n",
        "        clip_values=(0, 1),\n",
        "    )\n",
        "    # Custom training loop\n",
        "    train_acc = []\n",
        "    test_acc = []\n",
        "    for epoch in range(nb_epochs):\n",
        "        print(\"Epoch {}/{}\".format(epoch + 1, nb_epochs))\n",
        "        epoch_loss = []\n",
        "        for batch in range(0, len(x_train), batch_size):\n",
        "            batch_images = x_train[batch:batch + batch_size]\n",
        "            batch_labels = y_train[batch:batch + batch_size]\n",
        "            loss = train_step(model, optimizer, loss_object, batch_images, batch_labels)\n",
        "            epoch_loss.append(loss)\n",
        "        #print(epoch_loss)\n",
        "        avg_loss = np.mean(epoch_loss)\n",
        "        print(\"Average loss: {:.4f}\".format(avg_loss))\n",
        "\n",
        "        # Perform predictions and evaluate accuracy on examples\n",
        "        predictions = classifier.predict(x_test)\n",
        "        accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
        "        test_acc.append(accuracy)\n",
        "        print(\"Accuracy on test examples: {:.2%}\".format(accuracy))\n",
        "\n",
        "        predictions = classifier.predict(x_train)\n",
        "        accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_train, axis=1)) / len(y_train)\n",
        "        train_acc.append(accuracy)\n",
        "        print(\"Accuracy on train examples: {:.2%}\".format(accuracy))\n",
        "\n",
        "    return [classifier, test_acc, train_acc]\n"
      ],
      "metadata": {
        "id": "Vho0zupCqfl2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\n",
        "    create_art_classifier(model_creator=simple_FC, x_train=x_train, y_train=y_train, n_hidden=n_hidden),\n",
        "    create_art_classifier(model_creator=simple_Conv, x_train=x_train, y_train=y_train, n_hidden=n_hidden, kernel_size=kernel_size, padding_size=padding_size),\n",
        "    create_art_classifier(model_creator=simple_Conv_NL, x_train=x_train, y_train=y_train, n_hidden=n_hidden, kernel_size=kernel_size),\n",
        "    create_art_classifier(model_creator=simple_Conv_max, x_train=x_train, y_train=y_train, n_hidden=n_hidden, kernel_size=kernel_size),\n",
        "    create_art_classifier(model_creator=simple__RNN, x_train=x_train, y_train=y_train),\n",
        "    create_art_classifier(model_creator=simple_FC_2, x_train=x_train, y_train=y_train),\n",
        "    create_art_classifier(model_creator=simple_Conv_2, x_train=x_train, y_train=y_train)\n",
        "]\n"
      ],
      "metadata": {
        "id": "nMPaAnKzqnX4",
        "outputId": "e08b6d77-5734-446b-a103-ded797a18211",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "Average loss: 0.5185\n",
            "Accuracy on test examples: 85.14%\n",
            "Accuracy on train examples: 86.28%\n",
            "Epoch 2/30\n",
            "Average loss: 0.3807\n",
            "Accuracy on test examples: 85.12%\n",
            "Accuracy on train examples: 86.68%\n",
            "Epoch 3/30\n",
            "Average loss: 0.3382\n",
            "Accuracy on test examples: 85.74%\n",
            "Accuracy on train examples: 87.55%\n",
            "Epoch 4/30\n",
            "Average loss: 0.3120\n",
            "Accuracy on test examples: 86.62%\n",
            "Accuracy on train examples: 88.44%\n",
            "Epoch 5/30\n",
            "Average loss: 0.2935\n",
            "Accuracy on test examples: 86.90%\n",
            "Accuracy on train examples: 88.96%\n",
            "Epoch 6/30\n",
            "Average loss: 0.2778\n",
            "Accuracy on test examples: 87.61%\n",
            "Accuracy on train examples: 89.75%\n",
            "Epoch 7/30\n",
            "Average loss: 0.2655\n",
            "Accuracy on test examples: 87.61%\n",
            "Accuracy on train examples: 90.09%\n",
            "Epoch 8/30\n",
            "Average loss: 0.2526\n",
            "Accuracy on test examples: 87.71%\n",
            "Accuracy on train examples: 90.22%\n",
            "Epoch 9/30\n",
            "Average loss: 0.2423\n",
            "Accuracy on test examples: 87.85%\n",
            "Accuracy on train examples: 90.84%\n",
            "Epoch 10/30\n",
            "Average loss: 0.2333\n",
            "Accuracy on test examples: 87.90%\n",
            "Accuracy on train examples: 91.07%\n",
            "Epoch 11/30\n",
            "Average loss: 0.2240\n",
            "Accuracy on test examples: 88.03%\n",
            "Accuracy on train examples: 91.33%\n",
            "Epoch 12/30\n",
            "Average loss: 0.2158\n",
            "Accuracy on test examples: 88.21%\n",
            "Accuracy on train examples: 91.67%\n",
            "Epoch 13/30\n",
            "Average loss: 0.2080\n",
            "Accuracy on test examples: 88.16%\n",
            "Accuracy on train examples: 91.93%\n",
            "Epoch 14/30\n",
            "Average loss: 0.2012\n",
            "Accuracy on test examples: 87.97%\n",
            "Accuracy on train examples: 91.78%\n",
            "Epoch 15/30\n",
            "Average loss: 0.1941\n",
            "Accuracy on test examples: 88.35%\n",
            "Accuracy on train examples: 92.16%\n",
            "Epoch 16/30\n",
            "Average loss: 0.1882\n",
            "Accuracy on test examples: 88.62%\n",
            "Accuracy on train examples: 92.46%\n",
            "Epoch 17/30\n",
            "Average loss: 0.1826\n",
            "Accuracy on test examples: 88.93%\n",
            "Accuracy on train examples: 92.87%\n",
            "Epoch 18/30\n",
            "Average loss: 0.1768\n",
            "Accuracy on test examples: 88.87%\n",
            "Accuracy on train examples: 93.13%\n",
            "Epoch 19/30\n",
            "Average loss: 0.1716\n",
            "Accuracy on test examples: 88.98%\n",
            "Accuracy on train examples: 93.40%\n",
            "Epoch 20/30\n",
            "Average loss: 0.1667\n",
            "Accuracy on test examples: 89.10%\n",
            "Accuracy on train examples: 93.57%\n",
            "Epoch 21/30\n",
            "Average loss: 0.1616\n",
            "Accuracy on test examples: 89.18%\n",
            "Accuracy on train examples: 93.73%\n",
            "Epoch 22/30\n",
            "Average loss: 0.1573\n",
            "Accuracy on test examples: 89.07%\n",
            "Accuracy on train examples: 93.95%\n",
            "Epoch 23/30\n",
            "Average loss: 0.1534\n",
            "Accuracy on test examples: 89.18%\n",
            "Accuracy on train examples: 94.09%\n",
            "Epoch 24/30\n",
            "Average loss: 0.1495\n",
            "Accuracy on test examples: 89.16%\n",
            "Accuracy on train examples: 94.20%\n",
            "Epoch 25/30\n",
            "Average loss: 0.1458\n",
            "Accuracy on test examples: 89.26%\n",
            "Accuracy on train examples: 94.30%\n",
            "Epoch 26/30\n",
            "Average loss: 0.1419\n",
            "Accuracy on test examples: 89.24%\n",
            "Accuracy on train examples: 94.35%\n",
            "Epoch 27/30\n",
            "Average loss: 0.1386\n",
            "Accuracy on test examples: 89.26%\n",
            "Accuracy on train examples: 94.45%\n",
            "Epoch 28/30\n",
            "Average loss: 0.1352\n",
            "Accuracy on test examples: 89.20%\n",
            "Accuracy on train examples: 94.53%\n",
            "Epoch 29/30\n",
            "Average loss: 0.1320\n",
            "Accuracy on test examples: 89.37%\n",
            "Accuracy on train examples: 94.65%\n",
            "Epoch 30/30\n",
            "Average loss: 0.1289\n",
            "Accuracy on test examples: 89.30%\n",
            "Accuracy on train examples: 94.73%\n",
            "Epoch 1/30\n",
            "Average loss: 0.5602\n",
            "Accuracy on test examples: 83.89%\n",
            "Accuracy on train examples: 85.17%\n",
            "Epoch 2/30\n",
            "Average loss: 0.3918\n",
            "Accuracy on test examples: 84.83%\n",
            "Accuracy on train examples: 86.28%\n",
            "Epoch 3/30\n",
            "Average loss: 0.3503\n",
            "Accuracy on test examples: 85.86%\n",
            "Accuracy on train examples: 87.58%\n",
            "Epoch 4/30\n",
            "Average loss: 0.3252\n",
            "Accuracy on test examples: 86.43%\n",
            "Accuracy on train examples: 88.23%\n",
            "Epoch 5/30\n",
            "Average loss: 0.3065\n",
            "Accuracy on test examples: 86.20%\n",
            "Accuracy on train examples: 88.31%\n",
            "Epoch 6/30\n",
            "Average loss: 0.2910\n",
            "Accuracy on test examples: 86.80%\n",
            "Accuracy on train examples: 89.14%\n",
            "Epoch 7/30\n",
            "Average loss: 0.2778\n",
            "Accuracy on test examples: 86.95%\n",
            "Accuracy on train examples: 89.35%\n",
            "Epoch 8/30\n",
            "Average loss: 0.2677\n",
            "Accuracy on test examples: 87.26%\n",
            "Accuracy on train examples: 90.06%\n",
            "Epoch 9/30\n",
            "Average loss: 0.2572\n",
            "Accuracy on test examples: 87.62%\n",
            "Accuracy on train examples: 90.41%\n",
            "Epoch 10/30\n",
            "Average loss: 0.2481\n",
            "Accuracy on test examples: 87.82%\n",
            "Accuracy on train examples: 90.86%\n",
            "Epoch 11/30\n",
            "Average loss: 0.2391\n",
            "Accuracy on test examples: 87.95%\n",
            "Accuracy on train examples: 91.07%\n",
            "Epoch 12/30\n",
            "Average loss: 0.2312\n",
            "Accuracy on test examples: 88.28%\n",
            "Accuracy on train examples: 91.50%\n",
            "Epoch 13/30\n",
            "Average loss: 0.2233\n",
            "Accuracy on test examples: 88.36%\n",
            "Accuracy on train examples: 91.62%\n",
            "Epoch 14/30\n",
            "Average loss: 0.2166\n",
            "Accuracy on test examples: 88.71%\n",
            "Accuracy on train examples: 92.00%\n",
            "Epoch 15/30\n",
            "Average loss: 0.2097\n",
            "Accuracy on test examples: 88.74%\n",
            "Accuracy on train examples: 92.09%\n",
            "Epoch 16/30\n",
            "Average loss: 0.2042\n",
            "Accuracy on test examples: 88.75%\n",
            "Accuracy on train examples: 92.31%\n",
            "Epoch 17/30\n",
            "Average loss: 0.1989\n",
            "Accuracy on test examples: 88.88%\n",
            "Accuracy on train examples: 92.44%\n",
            "Epoch 18/30\n",
            "Average loss: 0.1938\n",
            "Accuracy on test examples: 88.91%\n",
            "Accuracy on train examples: 92.68%\n",
            "Epoch 19/30\n",
            "Average loss: 0.1887\n",
            "Accuracy on test examples: 88.73%\n",
            "Accuracy on train examples: 92.84%\n",
            "Epoch 20/30\n",
            "Average loss: 0.1844\n",
            "Accuracy on test examples: 88.73%\n",
            "Accuracy on train examples: 92.88%\n",
            "Epoch 21/30\n",
            "Average loss: 0.1799\n",
            "Accuracy on test examples: 88.83%\n",
            "Accuracy on train examples: 93.07%\n",
            "Epoch 22/30\n",
            "Average loss: 0.1759\n",
            "Accuracy on test examples: 88.81%\n",
            "Accuracy on train examples: 93.21%\n",
            "Epoch 23/30\n",
            "Average loss: 0.1720\n",
            "Accuracy on test examples: 88.92%\n",
            "Accuracy on train examples: 93.39%\n",
            "Epoch 24/30\n",
            "Average loss: 0.1686\n",
            "Accuracy on test examples: 89.07%\n",
            "Accuracy on train examples: 93.56%\n",
            "Epoch 25/30\n",
            "Average loss: 0.1646\n",
            "Accuracy on test examples: 89.08%\n",
            "Accuracy on train examples: 93.65%\n",
            "Epoch 26/30\n",
            "Average loss: 0.1610\n",
            "Accuracy on test examples: 89.20%\n",
            "Accuracy on train examples: 93.79%\n",
            "Epoch 27/30\n",
            "Average loss: 0.1581\n",
            "Accuracy on test examples: 89.05%\n",
            "Accuracy on train examples: 93.73%\n",
            "Epoch 28/30\n",
            "Average loss: 0.1550\n",
            "Accuracy on test examples: 88.90%\n",
            "Accuracy on train examples: 93.82%\n",
            "Epoch 29/30\n",
            "Average loss: 0.1516\n",
            "Accuracy on test examples: 88.93%\n",
            "Accuracy on train examples: 93.91%\n",
            "Epoch 30/30\n",
            "Average loss: 0.1489\n",
            "Accuracy on test examples: 88.97%\n",
            "Accuracy on train examples: 93.92%\n",
            "Epoch 1/30\n",
            "Average loss: 0.9859\n",
            "Accuracy on test examples: 69.72%\n",
            "Accuracy on train examples: 70.14%\n",
            "Epoch 2/30\n",
            "Average loss: 0.7098\n",
            "Accuracy on test examples: 72.95%\n",
            "Accuracy on train examples: 73.93%\n",
            "Epoch 3/30\n",
            "Average loss: 0.6517\n",
            "Accuracy on test examples: 76.26%\n",
            "Accuracy on train examples: 76.79%\n",
            "Epoch 4/30\n",
            "Average loss: 0.6158\n",
            "Accuracy on test examples: 77.07%\n",
            "Accuracy on train examples: 78.16%\n",
            "Epoch 5/30\n",
            "Average loss: 0.5890\n",
            "Accuracy on test examples: 78.27%\n",
            "Accuracy on train examples: 79.22%\n",
            "Epoch 6/30\n",
            "Average loss: 0.5683\n",
            "Accuracy on test examples: 78.94%\n",
            "Accuracy on train examples: 79.94%\n",
            "Epoch 7/30\n",
            "Average loss: 0.5520\n",
            "Accuracy on test examples: 79.36%\n",
            "Accuracy on train examples: 80.51%\n",
            "Epoch 8/30\n",
            "Average loss: 0.5388\n",
            "Accuracy on test examples: 79.92%\n",
            "Accuracy on train examples: 80.97%\n",
            "Epoch 9/30\n",
            "Average loss: 0.5277\n",
            "Accuracy on test examples: 80.45%\n",
            "Accuracy on train examples: 81.37%\n",
            "Epoch 10/30\n",
            "Average loss: 0.5180\n",
            "Accuracy on test examples: 80.81%\n",
            "Accuracy on train examples: 81.76%\n",
            "Epoch 11/30\n",
            "Average loss: 0.5095\n",
            "Accuracy on test examples: 81.20%\n",
            "Accuracy on train examples: 82.11%\n",
            "Epoch 12/30\n",
            "Average loss: 0.5020\n",
            "Accuracy on test examples: 81.51%\n",
            "Accuracy on train examples: 82.44%\n",
            "Epoch 13/30\n",
            "Average loss: 0.4953\n",
            "Accuracy on test examples: 81.77%\n",
            "Accuracy on train examples: 82.70%\n",
            "Epoch 14/30\n",
            "Average loss: 0.4892\n",
            "Accuracy on test examples: 82.07%\n",
            "Accuracy on train examples: 82.94%\n",
            "Epoch 15/30\n",
            "Average loss: 0.4836\n",
            "Accuracy on test examples: 82.17%\n",
            "Accuracy on train examples: 83.14%\n",
            "Epoch 16/30\n",
            "Average loss: 0.4786\n",
            "Accuracy on test examples: 82.41%\n",
            "Accuracy on train examples: 83.36%\n",
            "Epoch 17/30\n",
            "Average loss: 0.4741\n",
            "Accuracy on test examples: 82.56%\n",
            "Accuracy on train examples: 83.55%\n",
            "Epoch 18/30\n",
            "Average loss: 0.4699\n",
            "Accuracy on test examples: 82.65%\n",
            "Accuracy on train examples: 83.72%\n",
            "Epoch 19/30\n",
            "Average loss: 0.4661\n",
            "Accuracy on test examples: 82.76%\n",
            "Accuracy on train examples: 83.81%\n",
            "Epoch 20/30\n",
            "Average loss: 0.4625\n",
            "Accuracy on test examples: 82.79%\n",
            "Accuracy on train examples: 83.97%\n",
            "Epoch 21/30\n",
            "Average loss: 0.4592\n",
            "Accuracy on test examples: 82.88%\n",
            "Accuracy on train examples: 84.03%\n",
            "Epoch 22/30\n",
            "Average loss: 0.4561\n",
            "Accuracy on test examples: 83.01%\n",
            "Accuracy on train examples: 84.13%\n",
            "Epoch 23/30\n",
            "Average loss: 0.4531\n",
            "Accuracy on test examples: 83.10%\n",
            "Accuracy on train examples: 84.20%\n",
            "Epoch 24/30\n",
            "Average loss: 0.4504\n",
            "Accuracy on test examples: 83.18%\n",
            "Accuracy on train examples: 84.29%\n",
            "Epoch 25/30\n",
            "Average loss: 0.4477\n",
            "Accuracy on test examples: 83.27%\n",
            "Accuracy on train examples: 84.37%\n",
            "Epoch 26/30\n",
            "Average loss: 0.4451\n",
            "Accuracy on test examples: 83.32%\n",
            "Accuracy on train examples: 84.45%\n",
            "Epoch 27/30\n",
            "Average loss: 0.4427\n",
            "Accuracy on test examples: 83.35%\n",
            "Accuracy on train examples: 84.52%\n",
            "Epoch 28/30\n",
            "Average loss: 0.4403\n",
            "Accuracy on test examples: 83.47%\n",
            "Accuracy on train examples: 84.55%\n",
            "Epoch 29/30\n",
            "Average loss: 0.4379\n",
            "Accuracy on test examples: 83.46%\n",
            "Accuracy on train examples: 84.62%\n",
            "Epoch 30/30\n",
            "Average loss: 0.4357\n",
            "Accuracy on test examples: 83.61%\n",
            "Accuracy on train examples: 84.64%\n",
            "Epoch 1/30\n",
            "Average loss: 0.5845\n",
            "Accuracy on test examples: 85.62%\n",
            "Accuracy on train examples: 86.84%\n",
            "Epoch 2/30\n",
            "Average loss: 0.3593\n",
            "Accuracy on test examples: 86.53%\n",
            "Accuracy on train examples: 87.94%\n",
            "Epoch 3/30\n",
            "Average loss: 0.3208\n",
            "Accuracy on test examples: 87.27%\n",
            "Accuracy on train examples: 89.06%\n",
            "Epoch 4/30\n",
            "Average loss: 0.2923\n",
            "Accuracy on test examples: 87.43%\n",
            "Accuracy on train examples: 89.39%\n",
            "Epoch 5/30\n",
            "Average loss: 0.2709\n",
            "Accuracy on test examples: 87.85%\n",
            "Accuracy on train examples: 89.93%\n",
            "Epoch 6/30\n",
            "Average loss: 0.2544\n",
            "Accuracy on test examples: 87.93%\n",
            "Accuracy on train examples: 89.94%\n",
            "Epoch 7/30\n",
            "Average loss: 0.2408\n",
            "Accuracy on test examples: 87.73%\n",
            "Accuracy on train examples: 90.28%\n",
            "Epoch 8/30\n",
            "Average loss: 0.2288\n",
            "Accuracy on test examples: 87.90%\n",
            "Accuracy on train examples: 90.69%\n",
            "Epoch 9/30\n",
            "Average loss: 0.2199\n",
            "Accuracy on test examples: 88.31%\n",
            "Accuracy on train examples: 91.22%\n",
            "Epoch 10/30\n",
            "Average loss: 0.2090\n",
            "Accuracy on test examples: 88.75%\n",
            "Accuracy on train examples: 91.90%\n",
            "Epoch 11/30\n",
            "Average loss: 0.1991\n",
            "Accuracy on test examples: 88.82%\n",
            "Accuracy on train examples: 92.14%\n",
            "Epoch 12/30\n",
            "Average loss: 0.1917\n",
            "Accuracy on test examples: 89.39%\n",
            "Accuracy on train examples: 92.49%\n",
            "Epoch 13/30\n",
            "Average loss: 0.1847\n",
            "Accuracy on test examples: 89.14%\n",
            "Accuracy on train examples: 92.48%\n",
            "Epoch 14/30\n",
            "Average loss: 0.1791\n",
            "Accuracy on test examples: 89.42%\n",
            "Accuracy on train examples: 92.78%\n",
            "Epoch 15/30\n",
            "Average loss: 0.1719\n",
            "Accuracy on test examples: 89.32%\n",
            "Accuracy on train examples: 92.76%\n",
            "Epoch 16/30\n",
            "Average loss: 0.1663\n",
            "Accuracy on test examples: 89.48%\n",
            "Accuracy on train examples: 93.02%\n",
            "Epoch 17/30\n",
            "Average loss: 0.1603\n",
            "Accuracy on test examples: 89.56%\n",
            "Accuracy on train examples: 93.11%\n",
            "Epoch 18/30\n",
            "Average loss: 0.1554\n",
            "Accuracy on test examples: 89.60%\n",
            "Accuracy on train examples: 93.09%\n",
            "Epoch 19/30\n",
            "Average loss: 0.1530\n",
            "Accuracy on test examples: 89.78%\n",
            "Accuracy on train examples: 93.37%\n",
            "Epoch 20/30\n",
            "Average loss: 0.1523\n",
            "Accuracy on test examples: 90.07%\n",
            "Accuracy on train examples: 93.94%\n",
            "Epoch 21/30\n",
            "Average loss: 0.1508\n",
            "Accuracy on test examples: 89.89%\n",
            "Accuracy on train examples: 93.98%\n",
            "Epoch 22/30\n",
            "Average loss: 0.1449\n",
            "Accuracy on test examples: 89.94%\n",
            "Accuracy on train examples: 94.12%\n",
            "Epoch 23/30\n",
            "Average loss: 0.1408\n",
            "Accuracy on test examples: 90.14%\n",
            "Accuracy on train examples: 94.31%\n",
            "Epoch 24/30\n",
            "Average loss: 0.1373\n",
            "Accuracy on test examples: 90.24%\n",
            "Accuracy on train examples: 94.36%\n",
            "Epoch 25/30\n",
            "Average loss: 0.1340\n",
            "Accuracy on test examples: 90.20%\n",
            "Accuracy on train examples: 94.51%\n",
            "Epoch 26/30\n",
            "Average loss: 0.1309\n",
            "Accuracy on test examples: 90.08%\n",
            "Accuracy on train examples: 94.63%\n",
            "Epoch 27/30\n",
            "Average loss: 0.1284\n",
            "Accuracy on test examples: 90.24%\n",
            "Accuracy on train examples: 94.69%\n",
            "Epoch 28/30\n",
            "Average loss: 0.1269\n",
            "Accuracy on test examples: 89.93%\n",
            "Accuracy on train examples: 94.46%\n",
            "Epoch 29/30\n",
            "Average loss: 0.1236\n",
            "Accuracy on test examples: 89.93%\n",
            "Accuracy on train examples: 94.25%\n",
            "Epoch 30/30\n",
            "Average loss: 0.1194\n",
            "Accuracy on test examples: 89.83%\n",
            "Accuracy on train examples: 94.03%\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/backend.py:5561: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average loss: 0.7658\n",
            "Accuracy on test examples: 81.64%\n",
            "Accuracy on train examples: 82.13%\n",
            "Epoch 2/30\n",
            "Average loss: 0.4460\n",
            "Accuracy on test examples: 83.55%\n",
            "Accuracy on train examples: 84.55%\n",
            "Epoch 3/30\n",
            "Average loss: 0.3884\n",
            "Accuracy on test examples: 85.39%\n",
            "Accuracy on train examples: 86.22%\n",
            "Epoch 4/30\n",
            "Average loss: 0.3554\n",
            "Accuracy on test examples: 86.06%\n",
            "Accuracy on train examples: 87.16%\n",
            "Epoch 5/30\n",
            "Average loss: 0.3311\n",
            "Accuracy on test examples: 86.71%\n",
            "Accuracy on train examples: 88.09%\n",
            "Epoch 6/30\n",
            "Average loss: 0.3117\n",
            "Accuracy on test examples: 87.12%\n",
            "Accuracy on train examples: 88.90%\n",
            "Epoch 7/30\n",
            "Average loss: 0.2969\n",
            "Accuracy on test examples: 87.59%\n",
            "Accuracy on train examples: 89.61%\n",
            "Epoch 8/30\n",
            "Average loss: 0.2843\n",
            "Accuracy on test examples: 87.90%\n",
            "Accuracy on train examples: 89.95%\n",
            "Epoch 9/30\n",
            "Average loss: 0.2726\n",
            "Accuracy on test examples: 87.78%\n",
            "Accuracy on train examples: 90.01%\n",
            "Epoch 10/30\n",
            "Average loss: 0.2627\n",
            "Accuracy on test examples: 87.86%\n",
            "Accuracy on train examples: 90.11%\n",
            "Epoch 11/30\n",
            "Average loss: 0.2528\n",
            "Accuracy on test examples: 88.05%\n",
            "Accuracy on train examples: 90.28%\n",
            "Epoch 12/30\n",
            "Average loss: 0.2450\n",
            "Accuracy on test examples: 87.99%\n",
            "Accuracy on train examples: 90.52%\n",
            "Epoch 13/30\n",
            "Average loss: 0.2383\n",
            "Accuracy on test examples: 88.29%\n",
            "Accuracy on train examples: 90.85%\n",
            "Epoch 14/30\n",
            "Average loss: 0.2307\n",
            "Accuracy on test examples: 88.17%\n",
            "Accuracy on train examples: 90.88%\n",
            "Epoch 15/30\n",
            "Average loss: 0.2247\n",
            "Accuracy on test examples: 87.84%\n",
            "Accuracy on train examples: 90.82%\n",
            "Epoch 16/30\n",
            "Average loss: 0.2169\n",
            "Accuracy on test examples: 87.87%\n",
            "Accuracy on train examples: 90.73%\n",
            "Epoch 17/30\n",
            "Average loss: 0.2107\n",
            "Accuracy on test examples: 87.71%\n",
            "Accuracy on train examples: 90.89%\n",
            "Epoch 18/30\n",
            "Average loss: 0.2063\n",
            "Accuracy on test examples: 87.94%\n",
            "Accuracy on train examples: 91.08%\n",
            "Epoch 19/30\n",
            "Average loss: 0.1993\n",
            "Accuracy on test examples: 88.60%\n",
            "Accuracy on train examples: 91.90%\n",
            "Epoch 20/30\n",
            "Average loss: 0.1949\n",
            "Accuracy on test examples: 88.57%\n",
            "Accuracy on train examples: 92.27%\n",
            "Epoch 21/30\n",
            "Average loss: 0.1889\n",
            "Accuracy on test examples: 88.66%\n",
            "Accuracy on train examples: 92.50%\n",
            "Epoch 22/30\n",
            "Average loss: 0.1844\n",
            "Accuracy on test examples: 88.56%\n",
            "Accuracy on train examples: 92.60%\n",
            "Epoch 23/30\n",
            "Average loss: 0.1822\n",
            "Accuracy on test examples: 88.49%\n",
            "Accuracy on train examples: 92.73%\n",
            "Epoch 24/30\n",
            "Average loss: 0.1783\n",
            "Accuracy on test examples: 88.51%\n",
            "Accuracy on train examples: 93.06%\n",
            "Epoch 25/30\n",
            "Average loss: 0.1753\n",
            "Accuracy on test examples: 88.68%\n",
            "Accuracy on train examples: 93.01%\n",
            "Epoch 26/30\n",
            "Average loss: 0.1688\n",
            "Accuracy on test examples: 88.36%\n",
            "Accuracy on train examples: 92.69%\n",
            "Epoch 27/30\n",
            "Average loss: 0.1632\n",
            "Accuracy on test examples: 88.18%\n",
            "Accuracy on train examples: 92.69%\n",
            "Epoch 28/30\n",
            "Average loss: 0.1604\n",
            "Accuracy on test examples: 88.08%\n",
            "Accuracy on train examples: 92.60%\n",
            "Epoch 29/30\n",
            "Average loss: 0.1554\n",
            "Accuracy on test examples: 87.81%\n",
            "Accuracy on train examples: 92.44%\n",
            "Epoch 30/30\n",
            "Average loss: 0.1513\n",
            "Accuracy on test examples: 87.77%\n",
            "Accuracy on train examples: 92.45%\n",
            "Epoch 1/30\n",
            "Average loss: 0.6276\n",
            "Accuracy on test examples: 83.79%\n",
            "Accuracy on train examples: 84.98%\n",
            "Epoch 2/30\n",
            "Average loss: 0.4049\n",
            "Accuracy on test examples: 85.55%\n",
            "Accuracy on train examples: 86.59%\n",
            "Epoch 3/30\n",
            "Average loss: 0.3569\n",
            "Accuracy on test examples: 86.13%\n",
            "Accuracy on train examples: 87.40%\n",
            "Epoch 4/30\n",
            "Average loss: 0.3325\n",
            "Accuracy on test examples: 86.43%\n",
            "Accuracy on train examples: 88.05%\n",
            "Epoch 5/30\n",
            "Average loss: 0.3093\n",
            "Accuracy on test examples: 86.23%\n",
            "Accuracy on train examples: 88.29%\n",
            "Epoch 6/30\n",
            "Average loss: 0.2929\n",
            "Accuracy on test examples: 87.15%\n",
            "Accuracy on train examples: 89.13%\n",
            "Epoch 7/30\n",
            "Average loss: 0.2802\n",
            "Accuracy on test examples: 87.20%\n",
            "Accuracy on train examples: 89.58%\n",
            "Epoch 8/30\n",
            "Average loss: 0.2695\n",
            "Accuracy on test examples: 87.59%\n",
            "Accuracy on train examples: 90.09%\n",
            "Epoch 9/30\n",
            "Average loss: 0.2601\n",
            "Accuracy on test examples: 87.80%\n",
            "Accuracy on train examples: 90.50%\n",
            "Epoch 10/30\n",
            "Average loss: 0.2513\n",
            "Accuracy on test examples: 88.09%\n",
            "Accuracy on train examples: 90.75%\n",
            "Epoch 11/30\n",
            "Average loss: 0.2428\n",
            "Accuracy on test examples: 87.67%\n",
            "Accuracy on train examples: 90.48%\n",
            "Epoch 12/30\n",
            "Average loss: 0.2330\n",
            "Accuracy on test examples: 87.66%\n",
            "Accuracy on train examples: 90.48%\n",
            "Epoch 13/30\n",
            "Average loss: 0.2252\n",
            "Accuracy on test examples: 87.85%\n",
            "Accuracy on train examples: 90.78%\n",
            "Epoch 14/30\n",
            "Average loss: 0.2191\n",
            "Accuracy on test examples: 87.58%\n",
            "Accuracy on train examples: 90.66%\n",
            "Epoch 15/30\n",
            "Average loss: 0.2137\n",
            "Accuracy on test examples: 87.60%\n",
            "Accuracy on train examples: 90.88%\n",
            "Epoch 16/30\n",
            "Average loss: 0.2091\n",
            "Accuracy on test examples: 87.58%\n",
            "Accuracy on train examples: 90.80%\n",
            "Epoch 17/30\n",
            "Average loss: 0.2030\n",
            "Accuracy on test examples: 88.11%\n",
            "Accuracy on train examples: 91.36%\n",
            "Epoch 18/30\n",
            "Average loss: 0.1973\n",
            "Accuracy on test examples: 88.17%\n",
            "Accuracy on train examples: 91.60%\n",
            "Epoch 19/30\n",
            "Average loss: 0.1915\n",
            "Accuracy on test examples: 88.40%\n",
            "Accuracy on train examples: 91.86%\n",
            "Epoch 20/30\n",
            "Average loss: 0.1872\n",
            "Accuracy on test examples: 88.57%\n",
            "Accuracy on train examples: 92.17%\n",
            "Epoch 21/30\n",
            "Average loss: 0.1826\n",
            "Accuracy on test examples: 88.20%\n",
            "Accuracy on train examples: 92.01%\n",
            "Epoch 22/30\n",
            "Average loss: 0.1786\n",
            "Accuracy on test examples: 88.37%\n",
            "Accuracy on train examples: 92.20%\n",
            "Epoch 23/30\n",
            "Average loss: 0.1743\n",
            "Accuracy on test examples: 88.25%\n",
            "Accuracy on train examples: 92.35%\n",
            "Epoch 24/30\n",
            "Average loss: 0.1716\n",
            "Accuracy on test examples: 88.25%\n",
            "Accuracy on train examples: 92.46%\n",
            "Epoch 25/30\n",
            "Average loss: 0.1680\n",
            "Accuracy on test examples: 88.45%\n",
            "Accuracy on train examples: 92.81%\n",
            "Epoch 26/30\n",
            "Average loss: 0.1645\n",
            "Accuracy on test examples: 88.39%\n",
            "Accuracy on train examples: 92.92%\n",
            "Epoch 27/30\n",
            "Average loss: 0.1607\n",
            "Accuracy on test examples: 88.49%\n",
            "Accuracy on train examples: 93.02%\n",
            "Epoch 28/30\n",
            "Average loss: 0.1591\n",
            "Accuracy on test examples: 88.56%\n",
            "Accuracy on train examples: 92.92%\n",
            "Epoch 29/30\n",
            "Average loss: 0.1563\n",
            "Accuracy on test examples: 88.33%\n",
            "Accuracy on train examples: 92.82%\n",
            "Epoch 30/30\n",
            "Average loss: 0.1531\n",
            "Accuracy on test examples: 88.33%\n",
            "Accuracy on train examples: 92.86%\n",
            "Epoch 1/30\n",
            "Average loss: 0.6202\n",
            "Accuracy on test examples: 84.73%\n",
            "Accuracy on train examples: 85.75%\n",
            "Epoch 2/30\n",
            "Average loss: 0.3519\n",
            "Accuracy on test examples: 87.60%\n",
            "Accuracy on train examples: 88.61%\n",
            "Epoch 3/30\n",
            "Average loss: 0.2999\n",
            "Accuracy on test examples: 88.41%\n",
            "Accuracy on train examples: 89.36%\n",
            "Epoch 4/30\n",
            "Average loss: 0.2732\n",
            "Accuracy on test examples: 88.43%\n",
            "Accuracy on train examples: 89.96%\n",
            "Epoch 5/30\n",
            "Average loss: 0.2514\n",
            "Accuracy on test examples: 89.20%\n",
            "Accuracy on train examples: 90.96%\n",
            "Epoch 6/30\n",
            "Average loss: 0.2316\n",
            "Accuracy on test examples: 89.52%\n",
            "Accuracy on train examples: 91.49%\n",
            "Epoch 7/30\n",
            "Average loss: 0.2140\n",
            "Accuracy on test examples: 89.86%\n",
            "Accuracy on train examples: 92.07%\n",
            "Epoch 8/30\n",
            "Average loss: 0.1989\n",
            "Accuracy on test examples: 89.73%\n",
            "Accuracy on train examples: 92.48%\n",
            "Epoch 9/30\n",
            "Average loss: 0.1873\n",
            "Accuracy on test examples: 89.93%\n",
            "Accuracy on train examples: 92.94%\n",
            "Epoch 10/30\n",
            "Average loss: 0.1760\n",
            "Accuracy on test examples: 89.86%\n",
            "Accuracy on train examples: 92.91%\n",
            "Epoch 11/30\n",
            "Average loss: 0.1657\n",
            "Accuracy on test examples: 89.75%\n",
            "Accuracy on train examples: 93.06%\n",
            "Epoch 12/30\n",
            "Average loss: 0.1582\n",
            "Accuracy on test examples: 90.21%\n",
            "Accuracy on train examples: 93.87%\n",
            "Epoch 13/30\n",
            "Average loss: 0.1489\n",
            "Accuracy on test examples: 90.31%\n",
            "Accuracy on train examples: 94.39%\n",
            "Epoch 14/30\n",
            "Average loss: 0.1383\n",
            "Accuracy on test examples: 90.16%\n",
            "Accuracy on train examples: 94.65%\n",
            "Epoch 15/30\n",
            "Average loss: 0.1293\n",
            "Accuracy on test examples: 90.08%\n",
            "Accuracy on train examples: 94.88%\n",
            "Epoch 16/30\n",
            "Average loss: 0.1218\n",
            "Accuracy on test examples: 89.94%\n",
            "Accuracy on train examples: 94.88%\n",
            "Epoch 17/30\n",
            "Average loss: 0.1144\n",
            "Accuracy on test examples: 90.10%\n",
            "Accuracy on train examples: 94.99%\n",
            "Epoch 18/30\n",
            "Average loss: 0.1079\n",
            "Accuracy on test examples: 89.83%\n",
            "Accuracy on train examples: 94.97%\n",
            "Epoch 19/30\n",
            "Average loss: 0.1026\n",
            "Accuracy on test examples: 89.50%\n",
            "Accuracy on train examples: 94.59%\n",
            "Epoch 20/30\n",
            "Average loss: 0.0951\n",
            "Accuracy on test examples: 89.78%\n",
            "Accuracy on train examples: 95.14%\n",
            "Epoch 21/30\n",
            "Average loss: 0.0872\n",
            "Accuracy on test examples: 89.82%\n",
            "Accuracy on train examples: 95.66%\n",
            "Epoch 22/30\n",
            "Average loss: 0.0810\n",
            "Accuracy on test examples: 89.94%\n",
            "Accuracy on train examples: 96.13%\n",
            "Epoch 23/30\n",
            "Average loss: 0.0758\n",
            "Accuracy on test examples: 90.00%\n",
            "Accuracy on train examples: 96.28%\n",
            "Epoch 24/30\n",
            "Average loss: 0.0733\n",
            "Accuracy on test examples: 89.93%\n",
            "Accuracy on train examples: 96.27%\n",
            "Epoch 25/30\n",
            "Average loss: 0.0694\n",
            "Accuracy on test examples: 90.10%\n",
            "Accuracy on train examples: 96.52%\n",
            "Epoch 26/30\n",
            "Average loss: 0.0663\n",
            "Accuracy on test examples: 90.32%\n",
            "Accuracy on train examples: 96.80%\n",
            "Epoch 27/30\n",
            "Average loss: 0.0633\n",
            "Accuracy on test examples: 90.19%\n",
            "Accuracy on train examples: 96.95%\n",
            "Epoch 28/30\n",
            "Average loss: 0.0600\n",
            "Accuracy on test examples: 89.96%\n",
            "Accuracy on train examples: 97.00%\n",
            "Epoch 29/30\n",
            "Average loss: 0.0543\n",
            "Accuracy on test examples: 89.83%\n",
            "Accuracy on train examples: 96.80%\n",
            "Epoch 30/30\n",
            "Average loss: 0.0498\n",
            "Accuracy on test examples: 89.91%\n",
            "Accuracy on train examples: 96.81%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p saved_fashion_models"
      ],
      "metadata": {
        "id": "m-k4OFXErVF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, model in enumerate(models):\n",
        "    model_name = f'saved_fashion_models/model_{i}.h5'\n",
        "    model[0].model.save(model_name)"
      ],
      "metadata": {
        "id": "z6wGG6YirXMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array_models = np.array(models)\n",
        "acc_array = array_models[:, [1,2]]\n",
        "import pickle\n",
        "\n",
        "# Save the models list to a file\n",
        "with open('saved_fashion_models/acc_array.pkl', 'wb') as file:\n",
        "    pickle.dump(acc_array, file)"
      ],
      "metadata": {
        "id": "JJHHn2G8rjAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the models list to a file\n",
        "with open('saved_fashion_models/full_np.pkl', 'wb') as file:\n",
        "    pickle.dump(array_models, file)"
      ],
      "metadata": {
        "id": "urq0foVDrr5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/saved_fashion_models.zip /content/saved_fashion_models"
      ],
      "metadata": {
        "id": "dHcnubDXrvEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/saved_fashion_models.zip\")"
      ],
      "metadata": {
        "id": "4JITX5YkrxAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training and validation accuracy for each model\n",
        "plt.figure()\n",
        "for model, (_, test_acc, train_acc) in zip(models, model_names):\n",
        "    plt.plot(train_acc, label=model)\n",
        "    plt.plot(test_acc, label=model)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MpbuFY7_5Y-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable GPU acceleration for ART attacks\n",
        "from art.config import ART_NUMPY_DTYPE\n",
        "os.environ[\"ART_NUMPY_DTYPE\"] = str(ART_NUMPY_DTYPE)\n",
        "\n",
        "# Define the attack parameters\n",
        "attack_params = [[np.inf, [0.05, 0.1,  0.15, 0.2, 0.25, 0.3]],[2, [0.5, 1, 1.5,  2.5, 3]]]\n"
      ],
      "metadata": {
        "id": "CIlO3JiesF-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model, model_name in zip(models, model_names):\n",
        "    classifier = model[0]\n",
        "    # Iterate over the attack parameters and generate adversarial examples\n",
        "    for norm, epsilons in attack_params:\n",
        "        for epsilon in epsilons:\n",
        "            if norm == 2:\n",
        "                attack = FastGradientMethod(estimator=classifier, eps=epsilon, norm=norm)\n",
        "            else:\n",
        "                attack = ProjectedGradientDescentTensorFlowV2(estimator=classifier, eps=epsilon, norm=norm)\n",
        "\n",
        "            attack_name = attack.__class__.__name__\n",
        "            #model_name = \"simple_Conv_28_10_1000\"\n",
        "\n",
        "            adv_correct = 0\n",
        "            adv_loss = 0\n",
        "            total = 0\n",
        "            x_train_attack = []\n",
        "            y_train_attack = []\n",
        "            x_test_attack = []\n",
        "            y_test_attack = []\n",
        "\n",
        "            x_train_attack = attack.generate(x=x_train[:3000])\n",
        "            y_train_attack = np.copy(y_train[:3000])\n",
        "\n",
        "            x_test_attack = attack.generate(x=x_test[:3000])\n",
        "            y_test_attack = np.copy(y_test[:3000])\n",
        "\n",
        "            x_train_attack = np.array(x_train_attack)\n",
        "            y_train_attack = np.array(y_train_attack)\n",
        "            x_test_attack = np.array(x_test_attack)\n",
        "            y_test_attack = np.array(y_test_attack)\n",
        "\n",
        "            save_dir = \"adversarial_fashion_data\"\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "            np.savez(os.path.join(save_dir, f\"{model_name}_{attack_name}_{epsilon}_train.npz\"),\n",
        "                    x_train_attack=x_train_attack, y_train_attack=y_train_attack)\n",
        "            np.savez(os.path.join(save_dir, f\"{model_name}_{attack_name}_{epsilon}_test.npz\"),\n",
        "                    x_test_attack=x_test_attack, y_test_attack=y_test_attack)\n",
        "\n",
        "            print(os.path.join(save_dir, f\"{model_name}_{attack_name}_{epsilon}\"))\n",
        "\n",
        "            for x, y in tqdm(zip(x_test_attack, y_test_attack), total=len(y_test_attack),\n",
        "                            desc=\"Evaluating Adversarial Examples\"):\n",
        "                predictions_adv = np.argmax(classifier.predict(np.expand_dims(x, axis=0)), axis=1)\n",
        "                adv_correct += (predictions_adv == y).sum()\n",
        "                total += 1\n",
        "\n",
        "            _, adv_loss = classifier.model.evaluate(x_test_attack, y_test_attack, verbose=0)\n",
        "            accuracy = adv_correct / total\n",
        "            print(\"Accuracy on adversarial test examples (L_{:.0f}, eps={:.2f}): {:.2f}%. Loss: {:.2f}\".format(\n",
        "                norm, epsilon, accuracy * 100, adv_loss))"
      ],
      "metadata": {
        "id": "GjxjK1JisXa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/adversarial_fashion_data.zip /content/adversarial_fashion_data"
      ],
      "metadata": {
        "id": "ReG4xpjDshtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/adversaadversarial_fashion_datarial_data.zip\")"
      ],
      "metadata": {
        "id": "UL5zO0m-tKNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load the attack data\n",
        "attack_data = {}\n",
        "#attack_params = [[np.inf, [0.05, 0.1,  0.15, 0.2, 0.25, 0.3]],[2, [0.5, 1, 1.5,  2.5, 3]]]\n",
        "for model_name, model in zip(model_names,models):\n",
        "    for norm, epsilons in attack_params:\n",
        "        for epsilon in epsilons:\n",
        "            for data_type in [\"train\", \"test\"]:\n",
        "                if norm == np.inf:\n",
        "                    attack_name = \"ProjectedGradientDescentTensorFlowV2\"\n",
        "                else:\n",
        "                    attack_name = \"FastGradientMethod\"\n",
        "                file_name = f\"/content/adversarial_fashion_data/{model_name}_{attack_name}_{epsilon}_{data_type}.npz\"  # Modify the file name pattern as per your data\n",
        "                attack_data[(model_name, norm, epsilon,data_type)] = (model,np.load(file_name))\n"
      ],
      "metadata": {
        "id": "l13rcidFtSDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norms = [attack_params[0][0], 2]\n",
        "epsilons = {\n",
        "    attack_params[0][0]: attack_params[0][1],\n",
        "    attack_params[1][0]: attack_params[1][1]\n",
        "}"
      ],
      "metadata": {
        "id": "Re2h7FnYt3t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norms"
      ],
      "metadata": {
        "id": "motZhg33uPZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilons"
      ],
      "metadata": {
        "id": "IphfgQpwuQX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_names.keys()"
      ],
      "metadata": {
        "id": "2xVaXbZFu2Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# DO NOT USE MODELS\n",
        "models = ['simple_Conv_max', 'simple_Conv_NL', 'simple_Conv', 'simple_FC']\n",
        "\n",
        "# Create a dictionary to store the accuracy for each norm and model\n",
        "accuracy_data = {norm: {model: [] for model in models} for norm in norms}\n",
        "for (model_name, norm, epsilon, data_type), (model, data) in attack_data.items():\n",
        "    if data_type == \"test\":\n",
        "        model = model[0]\n",
        "        predictions = model.predict(x_test)\n",
        "        accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
        "        accuracy_data[norm][model_name].append((0, accuracy))\n",
        "    else:\n",
        "        continue"
      ],
      "metadata": {
        "id": "7yNNPXE5txRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (model_name, norm, epsilon, data_type), (model, data) in attack_data.items():\n",
        "    if data_type == \"test\":\n",
        "        model = model[0]\n",
        "        predictions = model.predict(data[f'x_{data_type}_attack'])\n",
        "        accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(data[f'y_{data_type}_attack'], axis=1)) / len(data[f'y_{data_type}_attack'])\n",
        "        accuracy_data[norm][model_name].append((epsilon, accuracy))\n",
        "    else:\n",
        "        continue"
      ],
      "metadata": {
        "id": "lQxIEopju9ME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for norm in norms:\n",
        "    plt.figure()\n",
        "    for model_name in models:\n",
        "        accuracies = accuracy_data[norm][model_name]\n",
        "        eps, accs = zip(*accuracies)\n",
        "        plt.plot(eps, accs, marker='o', label=model_name)\n",
        "    plt.xlabel('Epsilon')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title(f\"Accuracy on Attack Data (Norm={norm})\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WJ4GwbXwvMUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_shift_invariance(model, x_test, y_test, shifts):\n",
        "    model = model[0]\n",
        "    accuracies = []\n",
        "    for shift in tqdm(shifts):\n",
        "        shifted_images = shift_images(x_test, shift)\n",
        "        predictions = model.predict(shifted_images)\n",
        "        accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
        "        accuracies.append(accuracy)\n",
        "    return accuracies\n",
        "shifts = range(10,-11,-1)\n",
        "results = []\n",
        "for model in tqdm(models):\n",
        "    \n",
        "    accuracy = evaluate_shift_invariance(model, x_test, y_test, shifts)\n",
        "    results.append(accuracy)"
      ],
      "metadata": {
        "id": "nqqER7FrvFSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_data = {norm: {model: [] for model in models} for norm in norms}\n",
        "for (model_name, norm, epsilon, data_type), (model, data) in attack_data.items():\n",
        "    if data_type == \"train\":\n",
        "        model = model[0]\n",
        "        predictions = model.predict(x_train)\n",
        "        accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_train, axis=1)) / len(y_train)\n",
        "        accuracy_data[norm][model_name].append((0, accuracy))\n",
        "    else:\n",
        "        continue"
      ],
      "metadata": {
        "id": "VJc6YBCpvQ_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (model_name, norm, epsilon, data_type), (model, data) in attack_data.items():\n",
        "    if data_type == \"train\":\n",
        "        model = model[0]\n",
        "        predictions = model.predict(data[f'x_{data_type}_attack'])\n",
        "        accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(data[f'y_{data_type}_attack'], axis=1)) / len(data[f'y_{data_type}_attack'])\n",
        "        accuracy_data[norm][model_name].append((epsilon, accuracy))\n",
        "    else:\n",
        "        continue"
      ],
      "metadata": {
        "id": "MQbS5my_vUkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for norm in norms:\n",
        "    plt.figure()\n",
        "    for model_name in models:\n",
        "        accuracies = accuracy_data[norm][model_name]\n",
        "        eps, accs = zip(*accuracies)\n",
        "        plt.plot(eps, accs, marker='o', label=model_name)\n",
        "    plt.xlabel('Epsilon')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title(f\"Accuracy on Attack Data (Norm={norm})\")\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "mVt6-jj1vX2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_name_ = \"simple_Conv\"\n",
        "norm_ = np.inf\n",
        "epsilon_ = 0.15\n",
        "data_type_ = \"test\"\n",
        "accuracy_data = {model_name: {model: [] for model in models} for model_name in model_names.keys()}\n",
        "adversarial_data_x = {}\n",
        "adversarial_data_y = {}\n",
        "for original_model in model_names:\n",
        "    for (model_name, norm, epsilon, data_type), (model, data) in attack_data.items():\n",
        "        if (model_name, norm, epsilon, data_type) == (original_model, norm_, epsilon_, data_type_):\n",
        "            adversarial_data_x[original_model] = data[f'x_{data_type}_attack']\n",
        "            adversarial_data_y[original_model] = data[f'y_{data_type}_attack']\n",
        "        else:\n",
        "            continue\n",
        "for original_model in model_names:\n",
        "    for (model_name, norm, epsilon, data_type), (model, data) in attack_data.items():\n",
        "        if data_type == data_type_ and epsilon == epsilon_ and norm == norm_:\n",
        "            model = model[0]\n",
        "            predictions = model.predict(adversarial_data_x[original_model])\n",
        "            accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(adversarial_data_y[original_model], axis=1)) / len(adversarial_data_y[original_model])\n",
        "            accuracy_data[original_model][model_name].append((norm,epsilon,accuracy))\n",
        "        else:\n",
        "            continue"
      ],
      "metadata": {
        "id": "2qvRoZyivgP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for model_name, model_data in accuracy_data.items():\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    models = list(model_data.keys())\n",
        "    accuracies = [item[0][2] for item in model_data.values()]\n",
        "\n",
        "    ax.bar(models, accuracies)\n",
        "    ax.set_xlabel('Models')\n",
        "    ax.set_ylabel('Accuracy')\n",
        "    ax.set_title(f'Accuracy on Adversarial Data - Model: {model_name}')\n",
        "\n",
        "    # Rotate x-axis labels for better readability\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3BSAxizHvkyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def shift_images(images, shift, axis=0):\n",
        "    shifted_images = np.roll(images, shift, axis=axis)\n",
        "    return shifted_images\n",
        "\n",
        "def evaluate_shift_invariance(model, x_test, y_test, shifts, axis=0):\n",
        "    accuracies = []\n",
        "    for shift in tqdm(shifts):\n",
        "        shifted_images = shift_images(x_test, shift, axis=axis)\n",
        "        predictions = model.predict(shifted_images)\n",
        "        accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
        "        accuracies.append(accuracy)\n",
        "    return accuracies\n",
        "\n",
        "shifts = range(10, -11, -1)\n",
        "results = []\n",
        "\n",
        "# TO DO\n",
        "model_names = #['simple_FC', 'simple_Conv', 'simple_Conv_NL', 'simple_Conv_max']\n",
        "\n",
        "for model, model_name in zip(models, model_names):\n",
        "    model = model[0]\n",
        "    accuracy_axis0 = evaluate_shift_invariance(model, x_test, y_test, shifts, axis=0)\n",
        "    accuracy_axis1 = evaluate_shift_invariance(model, x_test, y_test, shifts, axis=1)\n",
        "    results.append((model_name, accuracy_axis0, accuracy_axis1))\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 10))\n",
        "\n",
        "for model_name, accuracy_axis0, accuracy_axis1 in results:\n",
        "    axes[0].plot(shifts, accuracy_axis0, label=model_name)\n",
        "    axes[1].plot(shifts, accuracy_axis1, label=model_name)\n",
        "\n",
        "axes[0].set_xlabel('Shift')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].set_title('Shift Invariance Evaluation (Axis 0)')\n",
        "\n",
        "axes[1].set_xlabel('Shift')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].legend()\n",
        "axes[1].set_title('Shift Invariance Evaluation (Axis 1)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "results_abs = []\n",
        "for model_k in range(len(results)):\n",
        "    shifts_abs = []\n",
        "    results_abs_k = []\n",
        "    for k in range(len(shifts)//2+1):\n",
        "        shifts_abs.append(shifts[-k-1])\n",
        "        results_abs_k.append((results[model_k][1][k] + results[model_k][1][-k-1]) / 2)\n",
        "    results_abs.append((results[model_k][0], results_abs_k))\n",
        "    \n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "for model_name, accuracy in results_abs:\n",
        "    ax.plot(shifts_abs, accuracy, label=model_name)\n",
        "\n",
        "ax.set_xlabel('Shift')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.legend()\n",
        "ax.set_title('Shift Invariance Evaluation (Symmetric Shifts)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lLLeUcTmv0Wa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Data Table Display",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}